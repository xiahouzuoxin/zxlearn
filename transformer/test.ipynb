{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer import Transformer, MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-9.2432e-02, -7.4963e-02, -3.9764e-02,  1.9147e-01,  2.6207e-02,\n",
       "          -4.1348e-02,  3.7420e-02,  7.5355e-02],\n",
       "         [-1.9543e-01, -2.4322e-01, -1.9727e-01,  1.7659e-01,  7.3606e-02,\n",
       "           7.7915e-02, -1.4432e-02,  9.0979e-02],\n",
       "         [-1.5270e-01, -2.5979e-01, -1.6798e-01,  1.9020e-01,  5.8147e-02,\n",
       "          -3.2706e-02,  2.1564e-03,  9.4873e-02]],\n",
       "\n",
       "        [[-6.8309e-02,  2.4378e-01,  1.3729e-01, -2.4940e-02,  4.1162e-02,\n",
       "           9.3957e-02,  8.1113e-02,  8.8998e-02],\n",
       "         [ 4.6381e-02,  2.9870e-01,  1.7701e-01,  4.3833e-02,  7.3284e-05,\n",
       "           6.6136e-02,  1.9146e-01,  9.2549e-02],\n",
       "         [-3.2271e-02,  2.6685e-01,  1.3234e-01, -1.8044e-02,  5.9349e-02,\n",
       "           5.3862e-02,  1.3626e-01,  1.0933e-01]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(2, 3, 8) # (batch, seq_len, emb_dim)\n",
    "\n",
    "MultiHeadAttention(d_model=8, num_heads=2, qkv_proj=True)(input, input, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[46, 85, 84, 32, 65],\n",
       "        [14, 98,  1, 24, 19],\n",
       "        [15, 85, 25, 77, 70],\n",
       "        [55, 86, 50, 43, 22],\n",
       "        [86, 37, 18, 52,  9],\n",
       "        [84, 82, 79, 70, 56],\n",
       "        [53, 88, 65, 53, 90],\n",
       "        [51, 47, 33, 21, 95],\n",
       "        [30, 87, 20, 48, 72],\n",
       "        [89,  8, 34, 14, 75]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_input = torch.randint(1, 100, (50, 5)) # (batch, seq_len)\n",
    "print(src_input.shape)\n",
    "src_input[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[32,  8, 25],\n",
       "        [ 5, 72, 69],\n",
       "        [10, 39, 57],\n",
       "        [65, 71, 24],\n",
       "        [67, 47, 47],\n",
       "        [36, 21, 17],\n",
       "        [88, 99, 29],\n",
       "        [47, 39, 17],\n",
       "        [37, 96, 31],\n",
       "        [54, 10, 87]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_input = torch.randint(1, 100, (50, 3)) # (batch, seq_len)\n",
    "print(tgt_input.shape)\n",
    "tgt_input[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=100, \n",
    "    d_model=32,     # the embedding dimension\n",
    "    num_heads=4,    # the number of heads in the multi-head attention\n",
    "    block_size=5,   # the maximum sequence length\n",
    "    num_encoders=6, # the number of encoders\n",
    "    num_decoders=6, # the number of decoders\n",
    "    bias=False,\n",
    "    dropout=0.1 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (input_embedding): InputEmbedding(\n",
       "    (tok_embed): Embedding(100, 32)\n",
       "    (pos_embed): PositionalEncoding()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder_blocks): ModuleList(\n",
       "    (0-5): 6 x EncoderBlock(\n",
       "      (qkv_proj): Linear(in_features=32, out_features=96, bias=False)\n",
       "      (attention): MultiHeadAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_fc): Linear(in_features=32, out_features=32, bias=False)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=32, out_features=128, bias=False)\n",
       "        (fc2): Linear(in_features=128, out_features=32, bias=False)\n",
       "      )\n",
       "      (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder_blocks): ModuleList(\n",
       "    (0-5): 6 x DecoderBlock(\n",
       "      (qkv_proj): Linear(in_features=32, out_features=96, bias=False)\n",
       "      (attention1): MultiHeadAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_fc): Linear(in_features=32, out_features=32, bias=False)\n",
       "      )\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (q_proj): Linear(in_features=32, out_features=32, bias=False)\n",
       "      (kv_proj): Linear(in_features=32, out_features=64, bias=False)\n",
       "      (attention2): MultiHeadAttention(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (output_fc): Linear(in_features=32, out_features=32, bias=False)\n",
       "      )\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=32, out_features=128, bias=False)\n",
       "        (fc2): Linear(in_features=128, out_features=32, bias=False)\n",
       "      )\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=32, out_features=100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3, 100])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(src_input, tgt_input)\n",
    "output.shape # (batch, seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0723,  0.1462,  0.7454,  0.2798,  0.4531,  0.2072, -0.1207,\n",
       "           0.1987,  1.3193,  0.0299],\n",
       "         [ 0.2442,  0.1691,  0.4099,  0.7136, -0.4226, -0.1607, -0.6516,\n",
       "           0.7076, -0.1966, -0.3897],\n",
       "         [ 1.0458,  0.0208, -0.5490, -0.1117,  0.3067,  1.0483, -0.5634,\n",
       "           0.7478,  0.2823,  0.5300]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:1,:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0080, 0.0086, 0.0157, 0.0098, 0.0117, 0.0091, 0.0066, 0.0091,\n",
       "          0.0278, 0.0077],\n",
       "         [0.0098, 0.0091, 0.0116, 0.0157, 0.0050, 0.0065, 0.0040, 0.0156,\n",
       "          0.0063, 0.0052],\n",
       "         [0.0210, 0.0075, 0.0043, 0.0066, 0.0100, 0.0211, 0.0042, 0.0156,\n",
       "          0.0098, 0.0125]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(output, dim=-1)[:1,:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
