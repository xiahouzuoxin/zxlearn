{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "df_samples = joblib.load('data/amazon_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192403"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['reviewerID'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63001"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['asin'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3526"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['brand'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['categories'].unique() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_configs = [\n",
    "    {\"name\": \"reviewerID\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3, \"hash_buckets\": 1000000},\n",
    "    {\"name\": \"asin\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3, \"hash_buckets\": 1000000},\n",
    "    \n",
    "    {\"name\": \"price\", \"dtype\": \"numerical\", \"norm\": \"std\"},\n",
    "    {\"name\": \"brand\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "    {\"name\": \"categories\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "\n",
    "    {\"name\": \"his_asin_seq\", \"dtype\": \"category\", \"islist\": True, \"min_freq\": 3, \"emb_dim\": 12, \"hash_buckets\": 1000000},\n",
    "]\n",
    "\n",
    "target_cols = ['label', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1352538 336650\n"
     ]
    }
   ],
   "source": [
    "from core.sample import traintest_split\n",
    "\n",
    "df_train, df_test = traintest_split(df_samples, test_size=0.2, shuffle=True, group_id='reviewerID')\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from core.dataset import FeatureTransformer\n",
    "\n",
    "# transformer = FeatureTransformer(feat_configs)\n",
    "\n",
    "# df_train = transformer.transform(df_train, is_train=True, n_jobs=4)\n",
    "# df_test = transformer.transform(df_test, is_train=False, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.dataset import DataFrameDataset\n",
    "\n",
    "train_dataset = DataFrameDataset(df_train, feat_configs, target_cols, is_raw=True, is_train=True, n_jobs=1, verbose=True)\n",
    "test_dataset = DataFrameDataset(df_test, feat_configs, target_cols, is_raw=True, is_train=False, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>overall</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>label</th>\n",
       "      <th>his_asin_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>314370</th>\n",
       "      <td>720607</td>\n",
       "      <td>942600</td>\n",
       "      <td>1375660800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>B+W 58mm Kaesemann Circular Polarizer with Mul...</td>\n",
       "      <td>0.118381</td>\n",
       "      <td>152</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538816</th>\n",
       "      <td>885984</td>\n",
       "      <td>597768</td>\n",
       "      <td>1342137600</td>\n",
       "      <td>5.0</td>\n",
       "      <td>OtterBox Defender Series Case with Screen Prot...</td>\n",
       "      <td>-0.075831</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194675</th>\n",
       "      <td>949944</td>\n",
       "      <td>277383</td>\n",
       "      <td>1397088000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Olympus VN-702PC Voice Recorder</td>\n",
       "      <td>-0.124243</td>\n",
       "      <td>53</td>\n",
       "      <td>139</td>\n",
       "      <td>1</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543055</th>\n",
       "      <td>63023</td>\n",
       "      <td>273198</td>\n",
       "      <td>1357516800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Manfrotto 701HDV Pro Fluid Video Mini Head</td>\n",
       "      <td>1.823619</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355447</th>\n",
       "      <td>604816</td>\n",
       "      <td>70388</td>\n",
       "      <td>1365724800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bear Motion Luxury Buffalo Hide Vintage Leathe...</td>\n",
       "      <td>-0.196901</td>\n",
       "      <td>545</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewerID    asin  unixReviewTime  overall  \\\n",
       "314370      720607  942600      1375660800      4.0   \n",
       "538816      885984  597768      1342137600      5.0   \n",
       "194675      949944  277383      1397088000      5.0   \n",
       "543055       63023  273198      1357516800      4.0   \n",
       "355447      604816   70388      1365724800      4.0   \n",
       "\n",
       "                                                    title     price  brand  \\\n",
       "314370  B+W 58mm Kaesemann Circular Polarizer with Mul...  0.118381    152   \n",
       "538816  OtterBox Defender Series Case with Screen Prot... -0.075831     76   \n",
       "194675                    Olympus VN-702PC Voice Recorder -0.124243     53   \n",
       "543055         Manfrotto 701HDV Pro Fluid Video Mini Head  1.823619      0   \n",
       "355447  Bear Motion Luxury Buffalo Hide Vintage Leathe... -0.196901    545   \n",
       "\n",
       "        categories  label                                       his_asin_seq  \n",
       "314370         196      1  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "538816           0      1  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "194675         139      1  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "543055         152      1  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "355447           0      1  [-100, -100, -100, -100, -100, -100, -100, -10...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=512, num_workers=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2642\n",
      "{'dense_features': tensor([[-0.5916]]), 'reviewerID': tensor([[146253]], dtype=torch.int32), 'asin': tensor([[826536]], dtype=torch.int32), 'brand': tensor([[633]], dtype=torch.int32), 'categories': tensor([[14]], dtype=torch.int32), 'his_asin_seq': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,  26408, 782092]], dtype=torch.int32)}\n",
      "tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "print( len(train_dataloader) )\n",
    "for features, labels in DataLoader(train_dataset,batch_size=1,shuffle=True):\n",
    "    print(features)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Model Input: dense_size=1, sparse_size=60\n",
      "DNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (reviewerID): Embedding(1000000, 12)\n",
      "    (asin): Embedding(1000000, 12)\n",
      "    (brand): Embedding(3419, 12)\n",
      "    (categories): Embedding(800, 12)\n",
      "    (his_asin_seq): Embedding(1000000, 12)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=61, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (logits): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import DNN\n",
    "\n",
    "dnn_hidden_units = [128,64,32]\n",
    "model = DNN(feat_configs, hidden_units=dnn_hidden_units)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),  lr = 0.001, weight_decay = 1e-9)\n",
    "lr_scd = lr_scheduler.StepLR(optimizer, step_size=len(train_dataloader), gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DNN:[Validation] Epoch: 0/5, Validation Loss: {'loss': 0.62591848538277}\n",
      "INFO:DNN:Learning rate: 0.001\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 0/2642, Training Loss: {'loss': 0.6200003027915955}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 100/2642, Training Loss: {'loss': 0.38717460542917254}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 200/2642, Training Loss: {'loss': 0.3737160398066044}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 300/2642, Training Loss: {'loss': 0.3683753237128258}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 400/2642, Training Loss: {'loss': 0.3657301820069552}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 500/2642, Training Loss: {'loss': 0.3627664979696274}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 600/2642, Training Loss: {'loss': 0.36207988058527313}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 700/2642, Training Loss: {'loss': 0.3600202789902687}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 800/2642, Training Loss: {'loss': 0.3585778087377548}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 900/2642, Training Loss: {'loss': 0.3577653518981404}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1000/2642, Training Loss: {'loss': 0.3570951369702816}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1100/2642, Training Loss: {'loss': 0.35604440648447383}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1200/2642, Training Loss: {'loss': 0.3552300176769495}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1300/2642, Training Loss: {'loss': 0.3551039589597629}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1400/2642, Training Loss: {'loss': 0.3545920719206333}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1500/2642, Training Loss: {'loss': 0.35388421243429186}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1600/2642, Training Loss: {'loss': 0.35339316485449673}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1700/2642, Training Loss: {'loss': 0.35291382831685686}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1800/2642, Training Loss: {'loss': 0.3524515644709269}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1900/2642, Training Loss: {'loss': 0.3520826874438085}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 2000/2642, Training Loss: {'loss': 0.3516124474555254}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 2100/2642, Training Loss: {'loss': 0.3512749145002592}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 2200/2642, Training Loss: {'loss': 0.35070482046766716}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 2300/2642, Training Loss: {'loss': 0.3505168453765952}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 2400/2642, Training Loss: {'loss': 0.3504393376285831}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 2500/2642, Training Loss: {'loss': 0.3501990840435028}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 2600/2642, Training Loss: {'loss': 0.34993439061137344}\n",
      "INFO:DNN:[Validation] Epoch: 1/5, Validation Loss: {'loss': 0.3396764594189664}\n",
      "INFO:DNN:Checkpoint saved at ./ckpt//checkpoint.best.002642.ckpt\n",
      "INFO:DNN:Learning rate: 0.0008\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 0/2642, Training Loss: {'loss': 0.33625397086143494}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 100/2642, Training Loss: {'loss': 0.34916527807712555}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 200/2642, Training Loss: {'loss': 0.3420009107887745}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 300/2642, Training Loss: {'loss': 0.3411686097582181}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 400/2642, Training Loss: {'loss': 0.3408595026284456}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 500/2642, Training Loss: {'loss': 0.3398697356581688}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 600/2642, Training Loss: {'loss': 0.33919227932890256}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 700/2642, Training Loss: {'loss': 0.3391483291557857}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 800/2642, Training Loss: {'loss': 0.3390530513599515}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 900/2642, Training Loss: {'loss': 0.33866747015052373}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1000/2642, Training Loss: {'loss': 0.3380441188812256}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1100/2642, Training Loss: {'loss': 0.3378502615473487}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1200/2642, Training Loss: {'loss': 0.3378411449243625}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1300/2642, Training Loss: {'loss': 0.3375303460772221}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1400/2642, Training Loss: {'loss': 0.3372230868254389}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1500/2642, Training Loss: {'loss': 0.33694220676024755}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1600/2642, Training Loss: {'loss': 0.33689287392422557}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1700/2642, Training Loss: {'loss': 0.3369815841141869}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1800/2642, Training Loss: {'loss': 0.33678631648421287}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1900/2642, Training Loss: {'loss': 0.3364541258310017}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 2000/2642, Training Loss: {'loss': 0.3363316025584936}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 2100/2642, Training Loss: {'loss': 0.3362725865131333}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 2200/2642, Training Loss: {'loss': 0.3361922476914796}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 2300/2642, Training Loss: {'loss': 0.3360257650328719}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 2400/2642, Training Loss: {'loss': 0.33599704841772715}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 2500/2642, Training Loss: {'loss': 0.33605227963924406}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 2600/2642, Training Loss: {'loss': 0.33590687450308065}\n",
      "INFO:DNN:[Validation] Epoch: 2/5, Validation Loss: {'loss': 0.33804944692049343}\n",
      "INFO:DNN:Checkpoint saved at ./ckpt//checkpoint.best.005284.ckpt\n",
      "INFO:DNN:Learning rate: 0.00064\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 0/2642, Training Loss: {'loss': 0.29322683811187744}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 100/2642, Training Loss: {'loss': 0.3275146463513374}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 200/2642, Training Loss: {'loss': 0.32786140725016594}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 300/2642, Training Loss: {'loss': 0.3272477634747823}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 400/2642, Training Loss: {'loss': 0.32704720057547093}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 500/2642, Training Loss: {'loss': 0.3266715355813503}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 600/2642, Training Loss: {'loss': 0.3262552766253551}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 700/2642, Training Loss: {'loss': 0.32595146734799657}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 800/2642, Training Loss: {'loss': 0.3248877933807671}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 900/2642, Training Loss: {'loss': 0.3246135988003678}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1000/2642, Training Loss: {'loss': 0.3243369175940752}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1100/2642, Training Loss: {'loss': 0.32439675321633166}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1200/2642, Training Loss: {'loss': 0.3240188718959689}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1300/2642, Training Loss: {'loss': 0.3237609973664467}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1400/2642, Training Loss: {'loss': 0.3237119729497603}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1500/2642, Training Loss: {'loss': 0.3234479037026564}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1600/2642, Training Loss: {'loss': 0.32323630728758873}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1700/2642, Training Loss: {'loss': 0.3230835800223491}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1800/2642, Training Loss: {'loss': 0.3230590521544218}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1900/2642, Training Loss: {'loss': 0.32318458750844004}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 2000/2642, Training Loss: {'loss': 0.3232187027856708}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 2100/2642, Training Loss: {'loss': 0.3229116061826547}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 2200/2642, Training Loss: {'loss': 0.3229124459962953}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 2300/2642, Training Loss: {'loss': 0.3228999861362188}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 2400/2642, Training Loss: {'loss': 0.32294479260221126}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 2500/2642, Training Loss: {'loss': 0.32293959046006204}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 2600/2642, Training Loss: {'loss': 0.3227456227174172}\n",
      "INFO:DNN:[Validation] Epoch: 3/5, Validation Loss: {'loss': 0.35025993705158176}\n",
      "INFO:DNN:Checkpoint saved at ./ckpt//checkpoint.007926.ckpt\n",
      "INFO:DNN:Learning rate: 0.0005120000000000001\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 0/2642, Training Loss: {'loss': 0.3100860118865967}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 100/2642, Training Loss: {'loss': 0.31213210433721544}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 200/2642, Training Loss: {'loss': 0.31155544131994245}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 300/2642, Training Loss: {'loss': 0.31259820918242137}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 400/2642, Training Loss: {'loss': 0.3117786991596222}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 500/2642, Training Loss: {'loss': 0.31245001405477524}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 600/2642, Training Loss: {'loss': 0.3127716722091039}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 700/2642, Training Loss: {'loss': 0.312474513905389}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 800/2642, Training Loss: {'loss': 0.3117562612146139}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 900/2642, Training Loss: {'loss': 0.31142032863365277}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1000/2642, Training Loss: {'loss': 0.31150441281497476}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1100/2642, Training Loss: {'loss': 0.311324688534845}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1200/2642, Training Loss: {'loss': 0.31124301346639793}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1300/2642, Training Loss: {'loss': 0.3111543830770713}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1400/2642, Training Loss: {'loss': 0.31117738785488264}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1500/2642, Training Loss: {'loss': 0.31125564765930175}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1600/2642, Training Loss: {'loss': 0.31103598564863205}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1700/2642, Training Loss: {'loss': 0.310820745115771}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1800/2642, Training Loss: {'loss': 0.31080187122854924}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1900/2642, Training Loss: {'loss': 0.3106055772226108}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 2000/2642, Training Loss: {'loss': 0.310568465821445}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 2100/2642, Training Loss: {'loss': 0.3105433135018462}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 2200/2642, Training Loss: {'loss': 0.31046940898353403}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 2300/2642, Training Loss: {'loss': 0.31040922062552495}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 2400/2642, Training Loss: {'loss': 0.3102012885486086}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 2500/2642, Training Loss: {'loss': 0.31012267174720765}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 2600/2642, Training Loss: {'loss': 0.31003768743803867}\n",
      "INFO:DNN:[Validation] Epoch: 4/5, Validation Loss: {'loss': 0.3557521548224075}\n",
      "INFO:DNN:Checkpoint saved at ./ckpt//checkpoint.010568.ckpt\n",
      "INFO:DNN:Early stopping at epoch 4...\n"
     ]
    }
   ],
   "source": [
    "from core.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scd,\n",
    "    max_epochs=5,\n",
    "    early_stopping_rounds=3,\n",
    "    save_ckpt_path='./ckpt/'\n",
    ")\n",
    "\n",
    "model = trainer.fit(train_dataloader, eval_dataloader = test_dataloader, ret_model = 'final') #, init_ckpt_path='./ckpt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DNN:Loaded model state_dict from checkpoint.\n",
      "INFO:DNN:Loaded model.training from checkpoint.\n",
      "INFO:DNN:Loaded model.feat_configs from checkpoint.\n",
      "INFO:DNN:Loaded optimizer = Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differ... from checkpoint.\n",
      "INFO:DNN:Loaded lr_scheduler = <torch.optim.lr_scheduler.StepLR object at 0x5d65357e0> from checkpoint.\n",
      "INFO:DNN:Loaded logger = <Logger DNN (INFO)> from checkpoint.\n",
      "INFO:DNN:Loaded default_ckpt_prefix = checkpoint from checkpoint.\n",
      "INFO:DNN:Loaded num_epoch = 4 from checkpoint.\n",
      "INFO:DNN:Loaded global_steps = 10568 from checkpoint.\n",
      "INFO:DNN:Loaded save_ckpt_path = ./ckpt/ from checkpoint.\n",
      "INFO:DNN:Loaded max_epochs = 5 from checkpoint.\n",
      "INFO:DNN:Loaded early_stopping_rounds = 3 from checkpoint.\n",
      "INFO:DNN:Checkpoint loaded from ./ckpt//checkpoint.010568.ckpt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = trainer.load_ckpt('checkpoint')\n",
    "model.load_state_dict(ckpt['model'].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "test_labels = []\n",
    "model.eval()\n",
    "\n",
    "for features, labels in test_dataloader:\n",
    "    outputs = model(features)\n",
    "    test_preds.append(outputs[:,0])\n",
    "    test_labels.append(labels[:,0])\n",
    "test_preds = torch.concat(test_preds, dim=0).detach().cpu().numpy()\n",
    "test_labels = torch.concat(test_labels, dim=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(336650,) (336650,)\n"
     ]
    }
   ],
   "source": [
    "print(test_preds.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.6675204951401648\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_score = roc_auc_score(test_labels, test_preds)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
