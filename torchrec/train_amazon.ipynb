{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "df_samples = joblib.load('data/amazon_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192403"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['reviewerID'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['asin'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3526"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['brand'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['categories'].unique() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hash buckets\n",
    "# feat_configs = [\n",
    "#     {\"name\": \"reviewerID\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3, \"hash_buckets\": 1000000},\n",
    "#     {\"name\": \"asin\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3, \"hash_buckets\": 1000000},\n",
    "    \n",
    "#     {\"name\": \"price\", \"dtype\": \"numerical\", \"norm\": \"std\"},\n",
    "#     {\"name\": \"brand\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "#     {\"name\": \"categories\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "\n",
    "#     {\"name\": \"his_asin_seq\", \"dtype\": \"category\", \"islist\": True, \"min_freq\": 3, \"emb_dim\": 12, \"hash_buckets\": 1000000},\n",
    "# ]\n",
    "\n",
    "## Dynamic Embedding\n",
    "feat_configs = [\n",
    "    {\"name\": \"reviewerID\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3},\n",
    "    {\"name\": \"asin\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3},\n",
    "    \n",
    "    {\"name\": \"price\", \"dtype\": \"numerical\", \"norm\": \"std\"},\n",
    "    {\"name\": \"brand\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "    {\"name\": \"categories\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "\n",
    "    {\"name\": \"his_asin_seq\", \"dtype\": \"category\", \"islist\": True, \"min_freq\": 3, \"emb_dim\": 12},\n",
    "]\n",
    "\n",
    "target_cols = ['label', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1352538 336650\n"
     ]
    }
   ],
   "source": [
    "from core.sample import traintest_split\n",
    "\n",
    "df_train, df_test = traintest_split(df_samples, test_size=0.2, shuffle=True, group_id='reviewerID')\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from core.dataset import FeatureTransformer\n",
    "\n",
    "# transformer = FeatureTransformer(feat_configs)\n",
    "\n",
    "# df_train = transformer.transform(df_train, is_train=True, n_jobs=4)\n",
    "# df_test = transformer.transform(df_test, is_train=False, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Feature transforming (is_train=True), note that feat_configs will be updated when is_train=True...\n",
      "Processing feature reviewerID...\n",
      "Converting category reviewerID to indices...\n",
      "Feature reviewerID vocab size: None -> 153923\n",
      "Processing feature asin...\n",
      "Converting category asin to indices...\n",
      "Feature asin vocab size: None -> 62384\n",
      "Processing feature price...\n",
      "Feature price mean: 74.35263034595687, std: 123.73053965875545, min: 0.01, max: 999.99\n",
      "Processing feature brand...\n",
      "Converting category brand to indices...\n",
      "Feature brand vocab size: None -> 3419\n",
      "Processing feature categories...\n",
      "Converting category categories to indices...\n",
      "Feature categories vocab size: None -> 800\n",
      "Processing feature his_asin_seq...\n",
      "Converting category his_asin_seq to indices...\n",
      "Feature his_asin_seq vocab size: None -> 61925\n",
      "==> Feature transforming (is_train=True) done...\n",
      "==> Dense features: ['price']\n",
      "==> Sparse features: ['reviewerID', 'asin', 'brand', 'categories']\n",
      "==> Sequence dense features: []\n",
      "==> Sequence sparse features: ['his_asin_seq']\n",
      "==> Weight columns mapping: {}\n",
      "==> Target columns: ['label']\n",
      "==> Finished dataset initialization, total samples: 1352538\n"
     ]
    }
   ],
   "source": [
    "from core.dataset import DataFrameDataset\n",
    "\n",
    "train_dataset = DataFrameDataset(df_train, feat_configs, target_cols, is_raw=True, is_train=True, n_jobs=1, verbose=True)\n",
    "test_dataset = DataFrameDataset(df_test, feat_configs, target_cols, is_raw=True, is_train=False, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3419"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max([v['idx'] for k,v in feat_configs[3]['vocab'].items()])\n",
    "# feat_configs[3]['num_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>overall</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>label</th>\n",
       "      <th>his_asin_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>314370</th>\n",
       "      <td>69777</td>\n",
       "      <td>1870</td>\n",
       "      <td>1375660800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>B+W 58mm Kaesemann Circular Polarizer with Mul...</td>\n",
       "      <td>0.118381</td>\n",
       "      <td>153</td>\n",
       "      <td>197</td>\n",
       "      <td>1</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538816</th>\n",
       "      <td>47184</td>\n",
       "      <td>140</td>\n",
       "      <td>1342137600</td>\n",
       "      <td>5.0</td>\n",
       "      <td>OtterBox Defender Series Case with Screen Prot...</td>\n",
       "      <td>-0.075831</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194675</th>\n",
       "      <td>111551</td>\n",
       "      <td>2570</td>\n",
       "      <td>1397088000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Olympus VN-702PC Voice Recorder</td>\n",
       "      <td>-0.124243</td>\n",
       "      <td>54</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543055</th>\n",
       "      <td>10936</td>\n",
       "      <td>6521</td>\n",
       "      <td>1357516800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Manfrotto 701HDV Pro Fluid Video Mini Head</td>\n",
       "      <td>1.823619</td>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355447</th>\n",
       "      <td>99991</td>\n",
       "      <td>3273</td>\n",
       "      <td>1365724800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bear Motion Luxury Buffalo Hide Vintage Leathe...</td>\n",
       "      <td>-0.196901</td>\n",
       "      <td>546</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewerID  asin  unixReviewTime  overall  \\\n",
       "314370       69777  1870      1375660800      4.0   \n",
       "538816       47184   140      1342137600      5.0   \n",
       "194675      111551  2570      1397088000      5.0   \n",
       "543055       10936  6521      1357516800      4.0   \n",
       "355447       99991  3273      1365724800      4.0   \n",
       "\n",
       "                                                    title     price  brand  \\\n",
       "314370  B+W 58mm Kaesemann Circular Polarizer with Mul...  0.118381    153   \n",
       "538816  OtterBox Defender Series Case with Screen Prot... -0.075831     77   \n",
       "194675                    Olympus VN-702PC Voice Recorder -0.124243     54   \n",
       "543055         Manfrotto 701HDV Pro Fluid Video Mini Head  1.823619      1   \n",
       "355447  Bear Motion Luxury Buffalo Hide Vintage Leathe... -0.196901    546   \n",
       "\n",
       "        categories  label                                       his_asin_seq  \n",
       "314370         197      1  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "538816           1      1  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "194675         140      1  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "543055         153      1  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "355447           1      1  [-100, -100, -100, -100, -100, -100, -100, -10...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=512, num_workers=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2642\n",
      "{'dense_features': tensor([[-0.5687]]), 'reviewerID': tensor([[25]], dtype=torch.int32), 'asin': tensor([[34173]], dtype=torch.int32), 'brand': tensor([[1]], dtype=torch.int32), 'categories': tensor([[70]], dtype=torch.int32), 'his_asin_seq': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,   531,  1710,  1170,  2701,   113,  6962,   143,\n",
      "          2644,  2538,  9005,  3661, 10189, 10085,  9201,  2916,  4782,  8519,\n",
      "           573,  8904,   450,   458,   464,    65,  2589,  1494,  1528, 10922,\n",
      "         12423]], dtype=torch.int32)}\n",
      "tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "print( len(train_dataloader) )\n",
    "for features, labels in DataLoader(train_dataset,batch_size=1,shuffle=True):\n",
    "    print(features)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Model Input: dense_size=1, sparse_size=60\n",
      "DNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (reviewerID): DynamicEmbedding(153923, 12)\n",
      "    (asin): DynamicEmbedding(62384, 12)\n",
      "    (brand): DynamicEmbedding(3420, 12)\n",
      "    (categories): DynamicEmbedding(800, 12)\n",
      "    (his_asin_seq): DynamicEmbedding(61926, 12)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=61, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (logits): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import DNN\n",
    "\n",
    "dnn_hidden_units = [128,64,32]\n",
    "model = DNN(feat_configs, hidden_units=dnn_hidden_units)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),  lr = 0.001, weight_decay = 1e-9)\n",
    "lr_scd = lr_scheduler.StepLR(optimizer, step_size=len(train_dataloader), gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Trainer:[Validation] Epoch: 0/5, Validation Loss: {'loss': 0.755312853399381}\n",
      "INFO:Trainer:Learning rate: 0.001\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 0/2642, Training Loss: {'loss': 0.7546861171722412}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 100/2642, Training Loss: {'loss': 0.4085532212257385}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 200/2642, Training Loss: {'loss': 0.38834586724638936}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 300/2642, Training Loss: {'loss': 0.37921916236480074}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 400/2642, Training Loss: {'loss': 0.37412176474928854}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 500/2642, Training Loss: {'loss': 0.3707314270734787}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 600/2642, Training Loss: {'loss': 0.3680782702565193}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 700/2642, Training Loss: {'loss': 0.36574090263673237}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 800/2642, Training Loss: {'loss': 0.36481436014175417}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 900/2642, Training Loss: {'loss': 0.36359754426611796}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1000/2642, Training Loss: {'loss': 0.3620089718699455}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1100/2642, Training Loss: {'loss': 0.3612548480792479}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1200/2642, Training Loss: {'loss': 0.36020261203249293}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1300/2642, Training Loss: {'loss': 0.3592456606259713}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1400/2642, Training Loss: {'loss': 0.35869747789842743}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1500/2642, Training Loss: {'loss': 0.3578956156770388}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1600/2642, Training Loss: {'loss': 0.35704896381124857}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1700/2642, Training Loss: {'loss': 0.35629194377099765}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1800/2642, Training Loss: {'loss': 0.3558368661834134}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1900/2642, Training Loss: {'loss': 0.35544100126153544}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2000/2642, Training Loss: {'loss': 0.35499305562675}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2100/2642, Training Loss: {'loss': 0.3543614178044455}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2200/2642, Training Loss: {'loss': 0.3540909578583457}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2300/2642, Training Loss: {'loss': 0.3536743210709613}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2400/2642, Training Loss: {'loss': 0.353474804336826}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2500/2642, Training Loss: {'loss': 0.3530573708415031}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2600/2642, Training Loss: {'loss': 0.35263563063282233}\n",
      "INFO:Trainer:[Validation] Epoch: 1/5, Validation Loss: {'loss': 0.34001708682909565}\n",
      "INFO:Trainer:Checkpoint saved at ./ckpt//checkpoint.002642.ckpt\n",
      "INFO:Trainer:Learning rate: 0.0008\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 0/2642, Training Loss: {'loss': 0.34218528866767883}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 100/2642, Training Loss: {'loss': 0.34380251973867415}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 200/2642, Training Loss: {'loss': 0.3408876897394657}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 300/2642, Training Loss: {'loss': 0.3407007835308711}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 400/2642, Training Loss: {'loss': 0.3396986617892981}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 500/2642, Training Loss: {'loss': 0.3395884607434273}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 600/2642, Training Loss: {'loss': 0.340109962473313}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 700/2642, Training Loss: {'loss': 0.3397504648566246}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 800/2642, Training Loss: {'loss': 0.3394819271937013}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 900/2642, Training Loss: {'loss': 0.34014592389265697}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1000/2642, Training Loss: {'loss': 0.3404938340783119}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1100/2642, Training Loss: {'loss': 0.34035556457259436}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1200/2642, Training Loss: {'loss': 0.34024856145183247}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1300/2642, Training Loss: {'loss': 0.33998904205285585}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1400/2642, Training Loss: {'loss': 0.3396550829069955}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1500/2642, Training Loss: {'loss': 0.3392014510234197}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1600/2642, Training Loss: {'loss': 0.33876330135390165}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1700/2642, Training Loss: {'loss': 0.3390792256067781}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1800/2642, Training Loss: {'loss': 0.3390069702102078}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1900/2642, Training Loss: {'loss': 0.33892904705122895}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2000/2642, Training Loss: {'loss': 0.338823278427124}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2100/2642, Training Loss: {'loss': 0.33844382009335927}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2200/2642, Training Loss: {'loss': 0.33823892676017503}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2300/2642, Training Loss: {'loss': 0.33812089881171353}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2400/2642, Training Loss: {'loss': 0.33788947557409604}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2500/2642, Training Loss: {'loss': 0.33771288208961486}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2600/2642, Training Loss: {'loss': 0.33751811990371117}\n",
      "INFO:Trainer:[Validation] Epoch: 2/5, Validation Loss: {'loss': 0.33738350637234454}\n",
      "INFO:Trainer:Checkpoint saved at ./ckpt//checkpoint.005284.ckpt\n",
      "INFO:Trainer:Learning rate: 0.00064\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 0/2642, Training Loss: {'loss': 0.3356485962867737}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 100/2642, Training Loss: {'loss': 0.3274798619747162}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 200/2642, Training Loss: {'loss': 0.32785597145557405}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 300/2642, Training Loss: {'loss': 0.3282152005036672}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 400/2642, Training Loss: {'loss': 0.3285180983692408}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 500/2642, Training Loss: {'loss': 0.32851748925447466}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 600/2642, Training Loss: {'loss': 0.328409573584795}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 700/2642, Training Loss: {'loss': 0.32760671607085634}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 800/2642, Training Loss: {'loss': 0.32762722462415694}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 900/2642, Training Loss: {'loss': 0.3274978819489479}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1000/2642, Training Loss: {'loss': 0.3270325388908386}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1100/2642, Training Loss: {'loss': 0.32681111557917164}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1200/2642, Training Loss: {'loss': 0.32665085171659786}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1300/2642, Training Loss: {'loss': 0.32658289476082875}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1400/2642, Training Loss: {'loss': 0.32621671654284}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1500/2642, Training Loss: {'loss': 0.3259897078772386}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1600/2642, Training Loss: {'loss': 0.3255849771853536}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1700/2642, Training Loss: {'loss': 0.32553794391891533}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1800/2642, Training Loss: {'loss': 0.3254637723747227}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1900/2642, Training Loss: {'loss': 0.3254078651964665}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2000/2642, Training Loss: {'loss': 0.3253334116861224}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2100/2642, Training Loss: {'loss': 0.32537456930393266}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2200/2642, Training Loss: {'loss': 0.3251438997618177}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2300/2642, Training Loss: {'loss': 0.32504994468844456}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2400/2642, Training Loss: {'loss': 0.32500402346253393}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2500/2642, Training Loss: {'loss': 0.3248177166223526}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2600/2642, Training Loss: {'loss': 0.3246315664167588}\n",
      "INFO:Trainer:[Validation] Epoch: 3/5, Validation Loss: {'loss': 0.3487405326772243}\n",
      "INFO:Trainer:Checkpoint saved at ./ckpt//checkpoint.007926.ckpt\n",
      "INFO:Trainer:Learning rate: 0.0005120000000000001\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 0/2642, Training Loss: {'loss': 0.3229927718639374}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 100/2642, Training Loss: {'loss': 0.3192587673664093}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 200/2642, Training Loss: {'loss': 0.3165471912920475}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 300/2642, Training Loss: {'loss': 0.3155441298087438}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 400/2642, Training Loss: {'loss': 0.31386569429188965}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 500/2642, Training Loss: {'loss': 0.31294730594754216}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 600/2642, Training Loss: {'loss': 0.31143723117808503}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 700/2642, Training Loss: {'loss': 0.3115565862613065}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 800/2642, Training Loss: {'loss': 0.3119131949730217}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 900/2642, Training Loss: {'loss': 0.3117156565189362}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1000/2642, Training Loss: {'loss': 0.3119049634039402}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1100/2642, Training Loss: {'loss': 0.3118952499330044}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1200/2642, Training Loss: {'loss': 0.3122551734621326}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1300/2642, Training Loss: {'loss': 0.31253222530851}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1400/2642, Training Loss: {'loss': 0.31259040354618006}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1500/2642, Training Loss: {'loss': 0.31271107189853986}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1600/2642, Training Loss: {'loss': 0.3125915586296469}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1700/2642, Training Loss: {'loss': 0.31248940274119374}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1800/2642, Training Loss: {'loss': 0.312542386394408}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1900/2642, Training Loss: {'loss': 0.31247132192316807}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2000/2642, Training Loss: {'loss': 0.31242906107753515}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2100/2642, Training Loss: {'loss': 0.3122820775068942}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2200/2642, Training Loss: {'loss': 0.3122643590718508}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2300/2642, Training Loss: {'loss': 0.31219470712801684}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2400/2642, Training Loss: {'loss': 0.312223107299457}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2500/2642, Training Loss: {'loss': 0.3120977420628071}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2600/2642, Training Loss: {'loss': 0.31205974710102263}\n",
      "INFO:Trainer:[Validation] Epoch: 4/5, Validation Loss: {'loss': 0.3590886487786893}\n",
      "INFO:Trainer:Checkpoint saved at ./ckpt//checkpoint.010568.ckpt\n",
      "INFO:Trainer:Early stopping at epoch 4...\n"
     ]
    }
   ],
   "source": [
    "from core.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scd,\n",
    "    max_epochs=5,\n",
    "    early_stopping_rounds=3,\n",
    "    save_ckpt_path='./ckpt/'\n",
    ")\n",
    "\n",
    "model = trainer.fit(train_dataloader, eval_dataloader = test_dataloader, ret_model = 'final') #, init_ckpt_path='./ckpt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Trainer:Loaded model state_dict from checkpoint.\n",
      "INFO:Trainer:Loaded model.training from checkpoint.\n",
      "INFO:Trainer:Loaded model.feat_configs from checkpoint.\n",
      "INFO:Trainer:Loaded optimizer = Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differ... from checkpoint.\n",
      "INFO:Trainer:Loaded lr_scheduler = <torch.optim.lr_scheduler.StepLR object at 0x2a9e546a0> from checkpoint.\n",
      "INFO:Trainer:Loaded logger = <Logger Trainer (INFO)> from checkpoint.\n",
      "INFO:Trainer:Loaded ckpt_file_prefix = checkpoint from checkpoint.\n",
      "INFO:Trainer:Loaded num_epoch = 4 from checkpoint.\n",
      "INFO:Trainer:Loaded global_steps = 10568 from checkpoint.\n",
      "INFO:Trainer:Loaded save_ckpt_path = ./ckpt/ from checkpoint.\n",
      "INFO:Trainer:Loaded metadata_fn = ./ckpt//metadata.json from checkpoint.\n",
      "INFO:Trainer:Loaded max_epochs = 5 from checkpoint.\n",
      "INFO:Trainer:Loaded early_stopping_rounds = 3 from checkpoint.\n",
      "INFO:Trainer:Checkpoint loaded from ./ckpt/checkpoint.010568.ckpt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = trainer.load_ckpt('./ckpt')\n",
    "model.load_state_dict(ckpt['model'].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "test_labels = []\n",
    "model.eval()\n",
    "\n",
    "for features, labels in test_dataloader:\n",
    "    outputs = model(features)\n",
    "    test_preds.append(outputs[:,0])\n",
    "    test_labels.append(labels[:,0])\n",
    "test_preds = torch.concat(test_preds, dim=0).detach().cpu().numpy()\n",
    "test_labels = torch.concat(test_labels, dim=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(336650,) (336650,)\n"
     ]
    }
   ],
   "source": [
    "print(test_preds.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.6673137778411582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_score = roc_auc_score(test_labels, test_preds)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
