{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "df_samples = joblib.load('data/amazon_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192403"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['reviewerID'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['asin'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3526"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['brand'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['categories'].unique() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_configs = [\n",
    "    {\"name\": \"reviewerID\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3, \"hash_buckets\": 1000000},\n",
    "    {\"name\": \"asin\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3, \"hash_buckets\": 1000000},\n",
    "    \n",
    "    {\"name\": \"price\", \"dtype\": \"numerical\", \"norm\": \"std\"},\n",
    "    {\"name\": \"brand\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "    {\"name\": \"categories\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "\n",
    "    {\"name\": \"his_asin_seq\", \"dtype\": \"category\", \"islist\": True, \"min_freq\": 3, \"emb_dim\": 12, \"hash_buckets\": 1000000},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1352538 336650\n"
     ]
    }
   ],
   "source": [
    "from core.sample import traintest_split\n",
    "\n",
    "df_train, df_test = traintest_split(df_samples, test_size=0.2, shuffle=True, group_id='reviewerID')\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Feature transforming (is_train=True), note that feat_configs will be updated when is_train=True...\n"
     ]
    }
   ],
   "source": [
    "from core.dataset import DataFrameDataset, feature_transform\n",
    "\n",
    "df_train = feature_transform(df_train, feat_configs, is_train=True)\n",
    "# feat_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>overall</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>label</th>\n",
       "      <th>his_asin_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>314370</th>\n",
       "      <td>720607</td>\n",
       "      <td>942600</td>\n",
       "      <td>1375660800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>B+W 58mm Kaesemann Circular Polarizer with Mul...</td>\n",
       "      <td>0.118381</td>\n",
       "      <td>150</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538816</th>\n",
       "      <td>885984</td>\n",
       "      <td>597768</td>\n",
       "      <td>1342137600</td>\n",
       "      <td>5.0</td>\n",
       "      <td>OtterBox Defender Series Case with Screen Prot...</td>\n",
       "      <td>-0.075831</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194675</th>\n",
       "      <td>949944</td>\n",
       "      <td>277383</td>\n",
       "      <td>1397088000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Olympus VN-702PC Voice Recorder</td>\n",
       "      <td>-0.124243</td>\n",
       "      <td>52</td>\n",
       "      <td>139</td>\n",
       "      <td>1</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543055</th>\n",
       "      <td>63023</td>\n",
       "      <td>273198</td>\n",
       "      <td>1357516800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Manfrotto 701HDV Pro Fluid Video Mini Head</td>\n",
       "      <td>1.823619</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355447</th>\n",
       "      <td>604816</td>\n",
       "      <td>70388</td>\n",
       "      <td>1365724800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bear Motion Luxury Buffalo Hide Vintage Leathe...</td>\n",
       "      <td>-0.196901</td>\n",
       "      <td>543</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-100, -100, -100, -100, -100, -100, -100, -10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewerID    asin  unixReviewTime  overall  \\\n",
       "314370      720607  942600      1375660800      4.0   \n",
       "538816      885984  597768      1342137600      5.0   \n",
       "194675      949944  277383      1397088000      5.0   \n",
       "543055       63023  273198      1357516800      4.0   \n",
       "355447      604816   70388      1365724800      4.0   \n",
       "\n",
       "                                                    title     price  brand  \\\n",
       "314370  B+W 58mm Kaesemann Circular Polarizer with Mul...  0.118381    150   \n",
       "538816  OtterBox Defender Series Case with Screen Prot... -0.075831     75   \n",
       "194675                    Olympus VN-702PC Voice Recorder -0.124243     52   \n",
       "543055         Manfrotto 701HDV Pro Fluid Video Mini Head  1.823619      0   \n",
       "355447  Bear Motion Luxury Buffalo Hide Vintage Leathe... -0.196901    543   \n",
       "\n",
       "        categories  label                                       his_asin_seq  \n",
       "314370         196      1  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "538816           0      1  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "194675         139      1  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "543055         152      1  [-100, -100, -100, -100, -100, -100, -100, -10...  \n",
       "355447           0      1  [-100, -100, -100, -100, -100, -100, -100, -10...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = feature_transform(df_test, feat_configs, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_cols = [f['name'] for f in feat_configs if f['type'] == 'sparse' and not f.get('islist')]\n",
    "seq_sparse_cols = [f['name'] for f in feat_configs if f['type'] == 'sparse' and f.get('islist')]\n",
    "dense_cols = ['price',]\n",
    "target_cols = ['label', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataFrameDataset(\n",
    "    df_train, \n",
    "    sparse_cols, \n",
    "    seq_sparse_cols, \n",
    "    dense_cols, \n",
    "    seq_dense_cols=None, \n",
    "    target_cols=target_cols, \n",
    "    padding_value=-100\n",
    ").to(device)\n",
    "\n",
    "test_dataset = DataFrameDataset(\n",
    "    df_test, \n",
    "    sparse_cols, \n",
    "    seq_sparse_cols, \n",
    "    dense_cols, \n",
    "    seq_dense_cols=None, \n",
    "    target_cols=target_cols, \n",
    "    padding_value=-100\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=512, num_workers=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2642\n",
      "{'dense_features': tensor([[-0.3832]]), 'reviewerID': tensor([[308619]], dtype=torch.int32), 'asin': tensor([[333654]], dtype=torch.int32), 'brand': tensor([[146]], dtype=torch.int32), 'categories': tensor([[12]], dtype=torch.int32), 'his_asin_seq': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100, 496395, 968111, 857706, 143025]], dtype=torch.int32)}\n",
      "tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "print( len(train_dataloader) )\n",
    "for features, labels in DataLoader(train_dataset,batch_size=1,shuffle=True):\n",
    "    print(features)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Model Input: dense_size=1, sparse_size=60\n",
      "DNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (reviewerID): Embedding(1000000, 12)\n",
      "    (asin): Embedding(1000000, 12)\n",
      "    (brand): Embedding(3417, 12)\n",
      "    (categories): Embedding(800, 12)\n",
      "    (his_asin_seq): Embedding(1000000, 12)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=61, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (logits): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import DNN\n",
    "\n",
    "dnn_hidden_units = [128,64,32]\n",
    "model = DNN(feat_configs, hidden_units=dnn_hidden_units)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),  lr = 0.001, weight_decay = 1e-9)\n",
    "lr_scd = lr_scheduler.StepLR(optimizer, step_size=len(train_dataloader), gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DNN:[Validation] Epoch: 0/5, Validation Loss: {'loss': 0.6848900155093532}\n",
      "INFO:DNN:Learning rate: 0.001\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 0/2642, Training Loss: {'loss': 0.6954851746559143}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 100/2642, Training Loss: {'loss': 0.40346105515956876}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 200/2642, Training Loss: {'loss': 0.3827251891791821}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 300/2642, Training Loss: {'loss': 0.37551273733377455}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 400/2642, Training Loss: {'loss': 0.37034794680774213}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 500/2642, Training Loss: {'loss': 0.3676031242609024}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 600/2642, Training Loss: {'loss': 0.3646525121231874}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 700/2642, Training Loss: {'loss': 0.3628039297035762}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 800/2642, Training Loss: {'loss': 0.3615765196830034}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 900/2642, Training Loss: {'loss': 0.36019112315442825}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1000/2642, Training Loss: {'loss': 0.3594335876405239}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1100/2642, Training Loss: {'loss': 0.3582686892693693}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1200/2642, Training Loss: {'loss': 0.35814436530073485}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1300/2642, Training Loss: {'loss': 0.3572918586089061}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1400/2642, Training Loss: {'loss': 0.35655682310462}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1500/2642, Training Loss: {'loss': 0.35590480585893}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1600/2642, Training Loss: {'loss': 0.3552963505685329}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1700/2642, Training Loss: {'loss': 0.3550838840709013}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1800/2642, Training Loss: {'loss': 0.35471961309512456}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 1900/2642, Training Loss: {'loss': 0.3540641619500361}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 2000/2642, Training Loss: {'loss': 0.353567158639431}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 2100/2642, Training Loss: {'loss': 0.35308512972933903}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 2200/2642, Training Loss: {'loss': 0.35287116406993435}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 2300/2642, Training Loss: {'loss': 0.35260605799115224}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 2400/2642, Training Loss: {'loss': 0.3522942172487577}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 2500/2642, Training Loss: {'loss': 0.3519403893351555}\n",
      "INFO:DNN:[Training] Epoch: 1/5 iter 2600/2642, Training Loss: {'loss': 0.3514052793727471}\n",
      "INFO:DNN:[Validation] Epoch: 1/5, Validation Loss: {'loss': 0.3405444181798802}\n",
      "INFO:DNN:Checkpoint saved at ./ckpt//checkpoint.best.2642.ckpt\n",
      "INFO:DNN:Learning rate: 0.0008\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 0/2642, Training Loss: {'loss': 0.3385884761810303}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 100/2642, Training Loss: {'loss': 0.34228451490402223}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 200/2642, Training Loss: {'loss': 0.33935328990221025}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 300/2642, Training Loss: {'loss': 0.34049224654833476}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 400/2642, Training Loss: {'loss': 0.3411406344175339}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 500/2642, Training Loss: {'loss': 0.3412473637461662}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 600/2642, Training Loss: {'loss': 0.34143928373853366}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 700/2642, Training Loss: {'loss': 0.341245473580701}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 800/2642, Training Loss: {'loss': 0.3410146477073431}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 900/2642, Training Loss: {'loss': 0.340962364408705}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1000/2642, Training Loss: {'loss': 0.34055878695845604}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1100/2642, Training Loss: {'loss': 0.3401659337769855}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1200/2642, Training Loss: {'loss': 0.3398494367301464}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1300/2642, Training Loss: {'loss': 0.3396305256164991}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1400/2642, Training Loss: {'loss': 0.33934600016900474}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1500/2642, Training Loss: {'loss': 0.33947894825538}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1600/2642, Training Loss: {'loss': 0.3394350963458419}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1700/2642, Training Loss: {'loss': 0.3387631437182426}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1800/2642, Training Loss: {'loss': 0.3383510075509548}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 1900/2642, Training Loss: {'loss': 0.3380984344451051}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 2000/2642, Training Loss: {'loss': 0.33792628706991673}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 2100/2642, Training Loss: {'loss': 0.33781437286308835}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 2200/2642, Training Loss: {'loss': 0.3378201049430804}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 2300/2642, Training Loss: {'loss': 0.3373958098110945}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 2400/2642, Training Loss: {'loss': 0.33717448585977156}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 2500/2642, Training Loss: {'loss': 0.3370238954424858}\n",
      "INFO:DNN:[Training] Epoch: 2/5 iter 2600/2642, Training Loss: {'loss': 0.33687820573265737}\n",
      "INFO:DNN:[Validation] Epoch: 2/5, Validation Loss: {'loss': 0.33692869870133674}\n",
      "INFO:DNN:Checkpoint saved at ./ckpt//checkpoint.best.5284.ckpt\n",
      "INFO:DNN:Learning rate: 0.00064\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 0/2642, Training Loss: {'loss': 0.3118719160556793}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 100/2642, Training Loss: {'loss': 0.3351459622383118}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 200/2642, Training Loss: {'loss': 0.3311424473673105}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 300/2642, Training Loss: {'loss': 0.32850570167104404}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 400/2642, Training Loss: {'loss': 0.32782797086983917}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 500/2642, Training Loss: {'loss': 0.3266499795019627}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 600/2642, Training Loss: {'loss': 0.3261492392172416}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 700/2642, Training Loss: {'loss': 0.3263664313086442}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 800/2642, Training Loss: {'loss': 0.32625312661752104}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 900/2642, Training Loss: {'loss': 0.3260328993035687}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1000/2642, Training Loss: {'loss': 0.3257143631130457}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1100/2642, Training Loss: {'loss': 0.32549337407404727}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1200/2642, Training Loss: {'loss': 0.32564235375573236}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1300/2642, Training Loss: {'loss': 0.3253895832598209}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1400/2642, Training Loss: {'loss': 0.32539558479828495}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1500/2642, Training Loss: {'loss': 0.32538736566901205}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1600/2642, Training Loss: {'loss': 0.3252309761662036}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1700/2642, Training Loss: {'loss': 0.32528667703270914}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1800/2642, Training Loss: {'loss': 0.32507957629859446}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 1900/2642, Training Loss: {'loss': 0.3249044118978475}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 2000/2642, Training Loss: {'loss': 0.3245078595429659}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 2100/2642, Training Loss: {'loss': 0.3243057185837201}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 2200/2642, Training Loss: {'loss': 0.32420295031233265}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 2300/2642, Training Loss: {'loss': 0.3240297576137211}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 2400/2642, Training Loss: {'loss': 0.3240364757801096}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 2500/2642, Training Loss: {'loss': 0.3238779086828232}\n",
      "INFO:DNN:[Training] Epoch: 3/5 iter 2600/2642, Training Loss: {'loss': 0.324007111203212}\n",
      "INFO:DNN:[Validation] Epoch: 3/5, Validation Loss: {'loss': 0.33789884025021527}\n",
      "INFO:DNN:Checkpoint saved at ./ckpt//checkpoint.7926.ckpt\n",
      "INFO:DNN:Learning rate: 0.0005120000000000001\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 0/2642, Training Loss: {'loss': 0.327406108379364}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 100/2642, Training Loss: {'loss': 0.31782608076930047}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 200/2642, Training Loss: {'loss': 0.31758839018642904}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 300/2642, Training Loss: {'loss': 0.3169016717374325}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 400/2642, Training Loss: {'loss': 0.31520436611026525}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 500/2642, Training Loss: {'loss': 0.31441159185767176}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 600/2642, Training Loss: {'loss': 0.3145625639955203}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 700/2642, Training Loss: {'loss': 0.31463406383991244}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 800/2642, Training Loss: {'loss': 0.31386278187856076}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 900/2642, Training Loss: {'loss': 0.3138516979912917}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1000/2642, Training Loss: {'loss': 0.3132034949809313}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1100/2642, Training Loss: {'loss': 0.312633074643937}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1200/2642, Training Loss: {'loss': 0.31224272498240074}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1300/2642, Training Loss: {'loss': 0.3121983689184372}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1400/2642, Training Loss: {'loss': 0.3125918421149254}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1500/2642, Training Loss: {'loss': 0.31239025641481083}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1600/2642, Training Loss: {'loss': 0.31217539819888773}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1700/2642, Training Loss: {'loss': 0.31192723174305526}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1800/2642, Training Loss: {'loss': 0.3117425971561008}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 1900/2642, Training Loss: {'loss': 0.31169048569704355}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 2000/2642, Training Loss: {'loss': 0.31186187152564526}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 2100/2642, Training Loss: {'loss': 0.311702154761269}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 2200/2642, Training Loss: {'loss': 0.31156558231873943}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 2300/2642, Training Loss: {'loss': 0.31160530606041786}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 2400/2642, Training Loss: {'loss': 0.3115529506653547}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 2500/2642, Training Loss: {'loss': 0.3116808286190033}\n",
      "INFO:DNN:[Training] Epoch: 4/5 iter 2600/2642, Training Loss: {'loss': 0.31155762111911406}\n",
      "INFO:DNN:[Validation] Epoch: 4/5, Validation Loss: {'loss': 0.341542015548535}\n",
      "INFO:DNN:Checkpoint saved at ./ckpt//checkpoint.10568.ckpt\n",
      "INFO:DNN:Early stopping at epoch 4...\n"
     ]
    }
   ],
   "source": [
    "from core.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scd,\n",
    "    max_epochs=5,\n",
    "    early_stopping_rounds=3,\n",
    "    save_ckpt_path='./ckpt/'\n",
    ")\n",
    "\n",
    "model = trainer.fit(train_dataloader, eval_dataloader = test_dataloader, ret_model = 'final') #, init_ckpt_path='./ckpt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DNN:Loaded model state_dict from checkpoint.\n",
      "INFO:DNN:Loaded model.training from checkpoint.\n",
      "INFO:DNN:Loaded model.feat_configs from checkpoint.\n",
      "INFO:DNN:Loaded optimizer = Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differ... from checkpoint.\n",
      "INFO:DNN:Loaded lr_scheduler = <torch.optim.lr_scheduler.StepLR object at 0x1547ceb90> from checkpoint.\n",
      "INFO:DNN:Loaded logger = <Logger DNN (INFO)> from checkpoint.\n",
      "INFO:DNN:Loaded default_ckpt_prefix = checkpoint from checkpoint.\n",
      "INFO:DNN:Loaded num_epoch = 4 from checkpoint.\n",
      "INFO:DNN:Loaded global_steps = 10568 from checkpoint.\n",
      "INFO:DNN:Loaded save_ckpt_path = ./ckpt/ from checkpoint.\n",
      "INFO:DNN:Loaded max_epochs = 5 from checkpoint.\n",
      "INFO:DNN:Loaded early_stopping_rounds = 3 from checkpoint.\n",
      "INFO:DNN:Checkpoint loaded from ./ckpt//checkpoint.10568.ckpt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = trainer.load_ckpt('checkpoint')\n",
    "model.load_state_dict(ckpt['model'].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "test_labels = []\n",
    "model.eval()\n",
    "\n",
    "for features, labels in test_dataloader:\n",
    "    outputs = model(features)\n",
    "    test_preds.append(outputs[:,0])\n",
    "    test_labels.append(labels[:,0])\n",
    "test_preds = torch.concat(test_preds, dim=0).detach().cpu().numpy()\n",
    "test_labels = torch.concat(test_labels, dim=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(336650,) (336650,)\n"
     ]
    }
   ],
   "source": [
    "print(test_preds.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.667654116026733\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_score = roc_auc_score(test_labels, test_preds)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
